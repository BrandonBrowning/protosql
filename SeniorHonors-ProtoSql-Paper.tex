\documentclass[titlepage]{article}

\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{indentfirst}
\usepackage{lipsum}
\usepackage{listings}
\usepackage{textcomp}
\usepackage{xcolor}

\lstset{breaklines=true}

\begin{document}

	\thispagestyle{empty}
	\topskip0pt
	\vspace*{\fill}
		~\centerline{\textcolor{red}{\Huge{WORK IN PROGRESS}}}
		\newline\newline

		\large{Known Issues}
		\begin{itemize}
			\item Adobe renders the 2.2 (The Parser) AST images oddly unless zoomed in.
			\item Fix code examples so it's still centered but left aligned.
			\item Should I act like my capstone project code (the optimizer) is a part of this project or not?  I assumed I was told it's essentially invisible to this project.
		\end{itemize}
	\vspace*{\fill}
	\newpage

	\title{Exploring Best Practices for the Creation of a Programming Language Ecosystem: A Case Study with ProtoSQL}
	\author{Brandon Browning}
	\maketitle

	\begin{abstract}

		Developing a programming language is a process full of managing many different forms of complexity.  This project is an experiment in managing the problems that may arise in the development of an ecosystem for a new programming language; it is meant to explore and evaluate the best practices through the creation of a full language.  This will result in an industrial-strength and full-featured tool chain, including a parser, optimizer, and compiler.  The parser will adhere to the language grammar, generate descriptive error messages, and supply an Abstract Syntax Tree (AST) to the optimizer.  The optimizer will then go through the tree and rewrite pieces as it sees fit, generally to improve performance.  Then the compiler, here a cross-compiler, will take the AST and output corresponding SQL code.

	\end{abstract}

	\section{Insight}

		This project started out with a deep curiosity into how languages are defined, and how people manage the tooling around them.  To somebody who is unfamiliar with workings of parsers and compilers, it seems like such an immense and magical process to have code that turns code into code.  It's always a bit magical, but I hope through this paper to show how the field of Computer Science has dealt with this problem, and why it's not as painful as one would think.

		First I'll go over some of the ideas behind programming languages; such as why new ones should be created, the ecosystem surrounding them, and how different ways they're implemented.  Then I'll describe the process behind the creation of ProtoSQL; including the difficulties faced, the benefits of some of the practices employed, and the overall experience and outcome of working on a new programming language.

	\section{Overview of Creating a New Language}

		Languages are usually created because there's some kind of problem one would like to solve; such as wanting to work at an abstraction level between two other languages, or a completely different way of thinking to some established standard.  Over time, the strengths and weaknesses of a language begin to show.  The weaknesses are not necessarily oversights by the original designer, but more intricacies that are brought out over time by heavy usage of the language.  Due to this phenomenon, the development of new languages serves an important role in the evolution of software.

		\subsection{The Grammar}

			The first part of the process towards the foundation of a new programming language, almost out of necessity, is to develop the syntax and semantics of how the language will function.  There's an already existing domain of problem that the creator wants to solve, so they must think on the constructs used to solve that problem, and how they will fit together.  For example, if somebody wanted to create a language for a hand-held algebra system, they'd have to consider brevity and ease of entry as design goals.  They may opt to limit the feature set and complexity of the language as it's probably aimed at beginners, and the limited input versus a desktop-based system would be a significant factor.  At this point, many of the details aren't known, but the high-level design starts coming in to play which heavily dictates the initial direction of the language.



		\subsection{The Parser}

			Usually the first part of a new programming language ecosystem to be implemented is the parser.  The construction of a parser helps solidify the correctness of the grammar, lets you start working on and provides a basis for the compiler, and helps iron out and explore the grammar.  If the language hasn't been carefully scoured over before work on the parser begins, then problems in the grammar naturally surface during work on the parser.

			Now what exactly does the parser do?  The parser is the engine that turns the source text, or the source material for the entire process, into a structure fed through the rest of the system until completion; this structure is known as an Abstract Syntax Tree (AST).  The AST is a representation of the grammar in the form of a manipulatable data structure.  The primary purpose of the parser is then to turn the source text into its corresponding AST.

			The AST could be visualized as a tree or graph data structure which is common in Computer Science, and where the name comes from; however, this is beyond the scope of the paper.  Essentially, It is a hierarchal structure of the pieces of the language that the source text demonstrates.  For example, here's a simple arithmetic expression:
			\newline

			~\centerline{\Large{$x + 5$}}
			\newline

			This could be represented graphically, slightly simplified, as the following..
			\footnote{AST Graphics generated from \href{http://www.gliffy.com}{gliffy.com} and manipulated with \href{http://www.getpaint.net/}{Paint.NET}}
			\newline

			~\centerline{\includegraphics[scale=.3]{ExampleSimpleAST.png}}
			\newline

			A more complex programming-language type expression might be:
			\newline

			~\centerline{\Large{$printf(''(\%d, \%d)'', x, \frac{1}{2})$}}
			\newline

			An AST that's much more similar to how the above expression would be treated in an industrial-strength parser would resemble the following..
			\newline

			~\centerline{\includegraphics[scale=.5]{ExampleCodeAST.png}}
			\newline

			Here, many of the important details left out of the previous graphic are shown.  The above AST does not have any information as to what plus, x, and 5 are.  Here, we see that at the highest level is a function being called, named printf, which has three arguments.  The type of the argument, as well as its value, are recorded.  Having the tree structure this way makes it quite easy to make optimizations.

			There's two main types of parser implementations; parser generators and less common parser combinators.  Parser generators convert a description of the grammar in a special format, mixed in with pieces of logic, into a new program which parses that grammar.  Parser generators are some of the first parsers made, and produce quite efficient code; however, there is an extra layer of indirection in producing the parser, and the input source code is generally a superset of the language the logic is written in.

			On the other hand, parser combinators are less often used, partially to being relatively new.  They contrast from parser generators by just being a library used to write your parser, not a proprietary extension, and there is no extra step of generating the parser; the code is the parser.  Parser combinators work differently from parser generators by building up large parsers from little parsers.  For example, one may create the functionality to turn any parser into a new parser that parsers a comma-separated list of the original parser.  So given a parser that can parse a number, for example:
			\newline

			~\centerline{$-42.3$}
			\newline

			the new parser will parse:
			\newline

			~\centerline{$-1, 13, 15.6, -.1$}
			\newline

			Then, given another function that parses the input parser surrounded by square brackets, you could generate a new parser resembling:
			\newline

			~\centerline{bracket(comma-seperated(bracket(comma-seperated(parse-number))))}
			\newline

			Which could parse the following:
			\newline

			~\centerline{$[[1, 2], [-1, 42.5], [1]]$}
			\newline

			Creating this parser took almost no effort, and as a bonus, all of the intermediate parsers are available to us.  This way, building up a parser for your grammar allows you to, for free, get all of the parsers you create on the way.  Everything is defined in a composable way, and it's exciting to put together mini-parsers to create a large system.

		\subsection{The Optimizer}

			Optimizers exist to improve the performance of the output.  Generally, a programming language is devised to address difficulties in other languages, and also to provide abstraction; optimizers are an important part in both roles; they can be smarter than previous languages, allowing some of the pains previously endured creating the old language's programs to disappear, and also to allow new abstractions to not come at a high cost.

			The optimizer would be a tool that transformers the AST generated by a parser into a more performant, similar AST, that should effectively behave exactly the same.  For example, the optimizer may crawl down the tree and find the addition operator being used on two constants.  For example:
			\newline

			~\centerline{$delay = duration() * 60 * 1000$}
			\newline

			Here, it may optimize away the addition and replace it with the only result possible, also known as inlining.  The new source may then have this line instead:
			\newline

			~\centerline{$delay = duration() * 60000$}
			\newline

			This is obviously quite a simple optimization, but it's easy to show and understand how the behavior of the program shouldn't differ from the previous version.  Optimizations like this, but much more complex, can save many fold of a program's execution time.

		\subsection{The Compiler}

			The role of the compiler is to translate the optimized AST into the target.  The target of a compiler is usually another language, whether it be one of the same abstraction level, or something much more low-level.  It does this by going through the AST, and deciding how it should be outputted.  Usually this is a recursive process, which when completed leads the the final output and end goal of the programming language; to end up in some other form.  Note the entire process from parsing through compilation is sometimes just known as compiling.

			For example, there may be a compilation system for converting an arithmetic expression from infix to postfix.  This problem could be solved by defining a grammar for infix arithmetic, then parsing it into an AST, and then traversing it in a way that will put the operations in post order.  Given an expression, such as:
			\newline

			~\centerline{\Large{$1 + \frac{5}{3}$}}
			\newline

			This could be represented as the following AST:
			\newline

			~\centerline{\includegraphics[scale=.5]{ExampleCompileBinaryAST.png}}
			\newline

			The compiler could then recursively traverse down the tree.  If it encounters a real number, it will just output it.  If it encounters a binary operator, however, it will recursively output its children, and then the operator name.  Here's a listing of how this process would occur:
			\newline

			Steps
			\begin{enumerate}
				\itemsep0em
				\item $compile(1 + \frac{5}{3})$
				\item $compile(1)\ compile(\frac{5}{3})\ +$
				\item $1\ (compile(5)\ compile(3)\ /)\ +$
				\item $1\ 5\ 3\ /\ +$
				\newline
			\end{enumerate}

	\section{Implementation of ProtoSQL}

		ProtoSQL began as a combination of two things; a curiosity into the inner-workings of programming languages, and a desire to find an alternative to SQL (a database manipulation language).  I've never worked on designing or implementing a language, so it's understandable why I didn't know how programming languages work.  However they're an extremely important part of the ecosystem, and I wanted to explore the way they transform text, make smart decisions, and translate it into another domain.  My desire to explore alternatives to SQL comes from a productivity point-of-view, not necessarily from a disagreement with its design; SQL is an old language, and I thought there'd be some ways to make development with it less painful.  It does, however, do its job very well; it just is quite unwieldy and verbose.

		\subsection{The Grammar}

			My main design goal was a succinct grammar that captured the common use-cases for SQL.  I was mostly targeting the exploratory SQL code written to hunt down problems, the ad-hoc queries that generally aren't usually kept around.  This subset of SQL code I felt was an important target since it always seemed to take excessive time, and it seemed there could be a better system.

			I began work on a grammar that stayed quite consistent throughout the process.  There were quite a few changes, however they were mostly things that were unimportant in the scheme of things, like what characters separate different constructs.  Some early pieces of ProtoSQL that worked successfully include:
			\newline

			Note: The ProtoSQL code will come after a caret (\textgreater) and a space (this is not necessary in the language itself).  Then the next line is the outputted SQL.

			~\centerline{\textgreater\ SomeTable}
			~\centerline{SELECT * FROM SomeTable;}
			\newline

			~\centerline{\textgreater\ SomeTable?id=5}
			~\centerline{SELECT * FROM SomeTable WHERE id = 5;}
			\newline

		\subsection{The Parser}

			The parser was the part I was most looking forward to, and in the end the part I thought was most interesting and exciting.  One of my side goals in this project was to work on the F\# programming language, so I chose the parser combinator library FParsec\footnote{Available at \url{http://www.quanttec.com/fparsec/}}, to provide the parsing primitives I'd use to build up my ProtoSQL parser.  This was essentially the standard parsing library for F\#, and it also seemed extremely interesting.

			Creating the initial low-feature parser was quite pleasant.  First I toyed around with the library testing how the different tools work, even creating a quite functional JSON\footnote{According to \url{http://www.json.org/}, `JSON (JavaScript Object Notation) is a lightweight data-interchange format'} parser as a test.  Then I began working on the ProtoSQL parser.  Given my lack of experience with FParsec, I still made a quite usable parser quickly for the initial grammar.  I used this advantage and ease of parser implementation to essentially experiment with my grammar; a luxury I feel may not always arise with other tool-chains.

			As I began advancing to working on the compiler, I sometimes had issues with my parser; however, in the end it was always a simple misunderstanding of myself trying to correctly express my grammar.  For example, with the following piece of code:
			\newline

			~\centerline{\textgreater\ SomeTable?x=5/y}
			\newline

			There was an ambiguous case that I did not consider.  I expected the forward slash to not act as division, but instead be an order-by clause.  There is in fact no ambiguity in a language, since the order that the clauses are defined is significant, so the parser logically went with the case that I essentially told it to use, which was to use division.  I felt if this could be confusing to users, let alone me, I should probably use a different sign for the separator for an order-by clause, so I changed it from `/' to `//'.  Therefore, the division case and then the order-by case are respectively:
			\newline

			~\centerline{\textgreater\ SomeTable?x=5/y}
			~\centerline{SELECT * FROM SomeTable WHERE x = (5 / y);}
			\newline

			~\centerline{\textgreater\ SomeTable?x=5//y}
			~\centerline{SELECT * FROM SomeTable WHERE x = 5 ORDER BY y ASCENDING;}
			\newline

		\subsection{The Optimizer}

			The optimizer for ProtoSQL is effectively nonexistent for a few reasons.  First of all, SQL is a highly efficient language from the point of view of the user.  As long as the database is configured well, the database will do all the work in making queries efficient.  This is due to the fact in SQL you essentially say \textbf{what} you want, not \textbf{how} you want it.  This makes SQL quite difficult, and futile to optimize.

			Instead of having an optimizer pass, the AST goes from the parser straight to the compiler.

		\subsection{The Compiler}

			To implement the compiler, I didn't use any other libraries or tools.  Essentially it involved smart work with text, and was more of a classical programming problem.  I had the least amount of problems working on the compiler, primarily due to the simplicity of SQL that I had to output.  Also, my AST format encodes all the information required, and the F\# language has many constructs making it easy to inspect the AST and generate the needed SQL code.  The style of my compiler essentially represents the one above in the overview.

	\section{Closing Thoughts}

		I thought this was an extremely exciting project, and am satisfied with the conclusion.  There's a unique feeling when I can enter any valid ProtoSQL code following my grammar, and it's magically transformed into the corresponding SQL I expect.  From here one could work on integrating the compilation system with an existing SQL tool, allowing you to essentially write ProtoSQL instead of SQL when doing ad-hoc database exploration and analysis.

		The code will be available open-source under the MIT license, which allows it to essentially be used for anything, but I am not liable for damages.  See the appendix for additional documents, such as the grammar.

	\section{Appendix}

		\subsection{Grammar Definition in EBNF}

\begin{lstlisting}
; ProtoSQL - A succinct language that cross-compiles to SQL
;  for a Senior Honors Project
;  by Brandon Browning

; Assumed symbols:
;   <EOF>
;   <int>
;   <float>
;   <sql-string-text> ; For simplicity, the allowed SQL text between single quote (')'s in a string
;   <newline>
;   <tab>
; Note: No strings are escaped; instead the above symbols are used

<proto-sql> ::= 
	<data-expr> <spacing>
	<where-clause>? <spacing>
	<order-by-clause>? <spacing>
	<select-clause>? <spacing>

<data-expr> ::= <join>+ | <table>
<join> ::= <table> <spacing> <join-arrow> <spacing> table> <join-tuple>?
<join-tuple> ::= "(" <spacing> <join-tuple-body> <spacing> ")"
<join-tuple-body> ::= <bracket-ident> (<spacing> "," <spacing> <bracket-ident>)?
<join-arrow> ::= "-->" | "-=>" | "-x>"

<where-clause> ::= <where>+v
<where> ::= "?" (<where-id> | <value-expr>)
<where-id> ::= <int> | <tuple>

<order-by-clause> ::= <order-by>+
<order-by> ::= <order-by-type> <opt-spacing> <column> <spacing>
<order-by-type> ::= "//" | "\\"

<select-clause> ::= ("{" <spacing> <select-body> <spacing> "}")
<select-body> ::= (<select> <spacing> ";" <spacing>)* <select> <spacing>
<select> ::= (<ident-or-bracket> <spacing> "=" <spacing>) <value-expr>

<column> ::= <tri-string>
<table> ::= <tri-string>
<tri-string> ::= <ident-or-bracket> (<spacing> "." <spacing> <ident-or-bracket>){0,2}

<tuple> ::= "(" <tuple-body> ")"
<tuple-body> ::= <value-expr> (<spacing> "," <spacing> <value-expr>)* <spacing> ","?

<value-expr> ::= <binary-op> | <f-call> | <primative>
<primative> ::=  <string> | <int> | <float> | <literal>
<string> ::= "'" <sql-string-text>  "'"
<literal> ::= <ident>
<binary-op> ::= <value-expr> <opt-spacing> <binary-op-name> <opt-spacing> <value-expr>
<binary-op-name> ::= "&&" | "||" | "<" | "<=" | ">" | ">=" | "~" | "!~" | "=" | "!=" | "%" | "*" | "/" | "+" | "-"
<f-call> ::= <ident> <tuple>

<spacing> ::= <space-or-newline>?
<space-or-newline> ::= <space> | <newline>
<space> ::= " " | <tab>

<ident-or-bracket> ::= <bracket-ident> | <ident>

<bracket-ident> ::= "[" <bracket-ident-part>+ "]"
<bracket-ident-part> ::= <digit-letter> | <bracket-ident-symbol>
<bracket-ident-symbol> ::= <space> | "." | "/" | "\"

<ident> ::= <ident-head> <ident-tail>
<ident-head> ::= "_" | <letter>
<ident-tail> ::= "" | <digit-letter> <ident-tail>

<digit-letter> ::= <letter> | <digit>
<digit> ::= "0" | "1" | "2" | ... | "9"
<letter> ::= <lower-letter> | <upper-letter>
<lower-letter> ::= "a" | "b" | ... | "z"
<upper-letter> ::=  "A" | "B" | ... | "Z"
\end{lstlisting}

\end{document}